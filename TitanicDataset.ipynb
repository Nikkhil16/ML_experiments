{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777a181-eef4-4115-8e54-45ee9130af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('train.csv')\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7279cf-c52e-4b02-97af-e8aa88db6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\", \"Embarked\", \"Survived\"]]\n",
    "titanic = titanic.dropna()\n",
    "titanic = titanic.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcefb7-c14d-4fee-9e1c-180c7a314d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "\n",
    "categorical_features = titanic[titanic.select_dtypes(include=['object']).columns.tolist()]\n",
    "numerical_features = titanic[titanic.select_dtypes(exclude=['object']).columns].drop('Survived', axis=1)\n",
    "label_features = titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98f2e3-d7eb-4d49-9023-b8ed671172db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "\n",
    "numerical_features_arr = MinMaxScaler().fit_transform(numerical_features)\n",
    "print (categorical_features)\n",
    "categorical_features_arr = OneHotEncoder().fit_transform(categorical_features).toarray()\n",
    "print (categorical_features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d5a1c4-7931-4d23-88cf-17bf1d160451",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)\n",
    "combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=1)\n",
    "combined_features = pd.concat([combined_features, label_features], axis=1).reset_index(drop=True)\n",
    "\n",
    "print (combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848baacb-a6d7-479b-b87c-ead934b4e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(combined_features, test_size=0.2, random_state=42)\n",
    "print (len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94f39e-14fe-40bc-8557-300d2b914eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H1, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred\n",
    "\n",
    "D_in, H1, D_out = 10, 8, 1\n",
    "lr = 0.01\n",
    "\n",
    "network = TwoLayerNet(D_in, H1, D_out)\n",
    "optimizer = optim.Adam(network.parameters(), lr)\n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1cded-708f-49bc-8e93-f6f80d63987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.DataFrame(data=train_data, columns=train_data.columns)\n",
    "loss_array = []\n",
    "                          \n",
    "for i in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    df_selected = train_data.iloc[:500]\n",
    "    X = df_selected.iloc[:, :-1].values\n",
    "    y = df_selected.iloc[:, -1].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    pred_y = network.forward(X_tensor)\n",
    "    pred = (pred_y >= 0.5).float()\n",
    "    loss = criterion(pred_y.squeeze(), y_tensor)\n",
    "    loss_array.append(loss.item())\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()  # Calculate the gradient of the loss\n",
    "    optimizer.step()  # Update the gradient\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32880d1-a238-4f19-b19a-db0047d5217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "wrong = 0\n",
    "df_selected = test_data.iloc[:100]\n",
    "X = df_selected.iloc[:, :-1].values\n",
    "y = df_selected.iloc[:, -1].values\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "pred_y = network.forward(X_tensor)\n",
    "pred = (pred_y >= 0.5).float()  # Since the answers are either 0 or 1, we need to set a threshold where >= 0.5 is 1 and < 0.5 is 0\n",
    "for i in range(100):\n",
    "    if pred[i].item() == y_tensor[i].item():\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "print (correct/(correct + wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00fbcd-4c61-4cd1-b86f-5301ed7a416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1) \n",
    "        self.linear2 = nn.Linear(H1, H2) \n",
    "        self.linear3 = nn.Linear(H2, H3) \n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        h_relu = F.relu(self.linear2(h_relu))\n",
    "        h_relu = F.relu(self.linear3(h_relu))\n",
    "        y_pred = self.sigmoid(self.linear4(h_relu))\n",
    "        return y_pred\n",
    "\n",
    "D_in, H1, H2, H3, D_out = 10, 32, 16, 8, 1\n",
    "lr = 0.01\n",
    "\n",
    "network = MultiLayerNet(D_in, H1, H2, H3, D_out)\n",
    "optimizer = optim.Adam(network.parameters(), lr)\n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6483ca-9562-4497-801c-121ca79dbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.DataFrame(data=train_data, columns=train_data.columns)\n",
    "loss_array = []\n",
    "                          \n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    df_selected = train_data.iloc[:500]\n",
    "    X = df_selected.iloc[:, :-1].values\n",
    "    y = df_selected.iloc[:, -1].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    pred_y = network.forward(X_tensor)\n",
    "    pred = (pred_y >= 0.5).float()\n",
    "    loss = criterion(pred_y.squeeze(), y_tensor)\n",
    "    loss_array.append(loss.item())\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()  # Calculate the gradient of the loss\n",
    "    optimizer.step()  # Update the gradient\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26f12c-f944-421a-aa20-4d5f6e933127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1) \n",
    "        self.linear2 = nn.Linear(H1, H2) \n",
    "        self.linear3 = nn.Linear(H2, H3) \n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        h_relu = F.relu(self.linear2(h_relu))\n",
    "        h_relu = F.relu(self.linear3(h_relu))\n",
    "        y_pred = self.sigmoid(self.linear4(h_relu))\n",
    "        return y_pred\n",
    "\n",
    "D_in, H1, H2, H3, D_out = 10, 32, 16, 8, 1\n",
    "lr = 0.005\n",
    "\n",
    "network = MultiLayerNet(D_in, H1, H2, H3, D_out)\n",
    "optimizer = optim.Adam(network.parameters(), lr)\n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.DataFrame(data=train_data, columns=train_data.columns)\n",
    "loss_array = []\n",
    "                          \n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    df_selected = train_data.iloc[:500]\n",
    "    X = df_selected.iloc[:, :-1].values\n",
    "    y = df_selected.iloc[:, -1].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    pred_y = network.forward(X_tensor)\n",
    "    pred = (pred_y >= 0.5).float()\n",
    "    loss = criterion(pred_y.squeeze(), y_tensor)\n",
    "    loss_array.append(loss.item())\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()  # Calculate the gradient of the loss\n",
    "    optimizer.step()  # Update the gradient\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fa5ca-abc8-4cbb-83b6-4c287715f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "wrong = 0\n",
    "df_selected = test_data.iloc[:100]\n",
    "X = df_selected.iloc[:, :-1].values\n",
    "y = df_selected.iloc[:, -1].values\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "pred_y = network.forward(X_tensor)\n",
    "pred = (pred_y >= 0.5).float()  # Since the answers are either 0 or 1, we need to set a threshold where >= 0.5 is 1 and < 0.5 is 0\n",
    "for i in range(100):\n",
    "    if pred[i].item() == y_tensor[i].item():\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "print (correct/(correct + wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0ff0b-f9c9-4fcf-b9d2-1b2f9da75467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MultiLayerNetDP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super(MultiLayerNetDP, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1) \n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3) \n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        h_relu = F.relu(self.linear2(h_relu))\n",
    "        h_relu = self.dropout(h_relu)\n",
    "        h_relu = F.relu(self.linear3(h_relu))\n",
    "        h_relu = self.dropout(h_relu)\n",
    "        y_pred = self.sigmoid(self.linear4(h_relu))\n",
    "        return y_pred\n",
    "\n",
    "D_in, H1, H2, H3, D_out = 10, 32, 16, 8, 1\n",
    "lr = 0.005\n",
    "\n",
    "network = MultiLayerNet(D_in, H1, H2, H3, D_out)\n",
    "optimizer = optim.Adam(network.parameters(), lr)\n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dde1d-2960-450c-b9fc-b2b5a83b9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.DataFrame(data=train_data, columns=train_data.columns)\n",
    "loss_array = []\n",
    "                          \n",
    "for i in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    df_selected = train_data.iloc[:500]\n",
    "    X = df_selected.iloc[:, :-1].values\n",
    "    y = df_selected.iloc[:, -1].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    pred_y = network.forward(X_tensor)\n",
    "    pred = (pred_y >= 0.5).float()\n",
    "    loss = criterion(pred_y.squeeze(), y_tensor)\n",
    "    loss_array.append(loss.item())\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()  # Calculate the gradient of the loss\n",
    "    optimizer.step()  # Update the gradient\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152df21-5056-4a9a-ba5d-1fb2a48d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=0\n",
    "wrong = 0\n",
    "df_selected = test_data.iloc[:100]\n",
    "X = df_selected.iloc[:, :-1].values\n",
    "y = df_selected.iloc[:, -1].values\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "pred_y = network.forward(X_tensor)\n",
    "pred = (pred_y >= 0.5).float()  # Since the answers are either 0 or 1, we need to set a threshold where >= 0.5 is 1 and < 0.5 is 0\n",
    "for i in range(100):\n",
    "    if pred[i].item() == y_tensor[i].item():\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "print (correct/(correct + wrong))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04872e07-90a6-46b4-a573-e2c5177aab64",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571043f-0cd8-4f6b-8f2b-58490060c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_data)\n",
    "\n",
    "sns.heatmap(train_data[['Survived', 'Pclass', 'SibSp', 'Parch', 'Age', 'Fare', 0, 1, 2, 3, 4]].corr(), annot = True, fmt = '.2f', cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea8bb8-a932-4f44-ba95-1163918e5351",
   "metadata": {},
   "source": [
    "#### Make fare less skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6817c7-ad4e-467c-9ed1-0f08daa6eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_data['Fare'], label = 'Skewness: %.2f'%(train_data['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Fare Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a59ac-8329-4894-bb2f-5270b2808e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data['Fare'] = train_data['Fare'].map(lambda x: np.log(x) if x > 0 else 0)\n",
    "sns.distplot(train_data['Fare'], label = 'Skewness: %.2f'%(train_data['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Fare Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f38249-b807-4b3c-bab2-443e351b19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\"\n",
    "    This function will loop through a list of features and detect outliers in each one of those features. In each\n",
    "    loop, a data point is deemed an outlier if it is less than the first quartile minus the outlier step or exceeds\n",
    "    third quartile plus the outlier step. The outlier step is defined as 1.5 times the interquartile range. Once the \n",
    "    outliers have been determined for one feature, their indices will be stored in a list before proceeding to the next\n",
    "    feature and the process repeats until the very last feature is completed. Finally, using the list with outlier \n",
    "    indices, we will count the frequencies of the index numbers and return them if their frequency exceeds n times.    \n",
    "    \"\"\"\n",
    "    outlier_indices = [] \n",
    "    for col in features: \n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_step = 1.5 * IQR \n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n",
    "        outlier_indices.extend(outlier_list_col) \n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n",
    "    return multiple_outliers\n",
    "\n",
    "outliers_to_drop = detect_outliers(train_data, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\n",
    "print(\"We will drop these {} indices: \".format(len(outliers_to_drop)), outliers_to_drop)\n",
    "\n",
    "train_data.loc[outliers_to_drop, :]\n",
    "train_data = train_data.drop(outliers_to_drop, axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc091808-574c-4089-96f4-4cd3b471a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = train_data.drop(2, axis = 1)\n",
    "new_train_data = new_train_data.drop(3, axis = 1)\n",
    "new_train_data = new_train_data.drop(4, axis = 1)\n",
    "new_train_data = new_train_data.drop('SibSp', axis = 1)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1) # this will create weight, bias for linear1\n",
    "        self.linear2 = nn.Linear(H1, D_out) # this will create weight, bias for linear2\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.sigmoid(self.linear2(h_relu))\n",
    "        return y_pred\n",
    "\n",
    "D_in, H1, D_out = 6, 3, 1\n",
    "lr = 0.005\n",
    "\n",
    "network = TwoLayerNet(D_in, H1, D_out)\n",
    "optimizer = optim.Adam(network.parameters(), lr)\n",
    "criterion = nn.BCELoss() # Define the loss function as Binary Cross-Entropy Loss\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = pd.DataFrame(data=train_data, columns=train_data.columns)\n",
    "loss_array = []\n",
    "                          \n",
    "for i in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    df_selected = new_train_data.iloc[:500]\n",
    "    X = df_selected.iloc[:, :-1].values\n",
    "    y = df_selected.iloc[:, -1].values\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    pred_y = network.forward(X_tensor)\n",
    "    pred = (pred_y >= 0.5).float()\n",
    "    loss = criterion(pred_y.squeeze(), y_tensor)\n",
    "    loss_array.append(loss.item())\n",
    "\n",
    "    # Backward propagation\n",
    "    loss.backward()  # Calculate the gradient of the loss\n",
    "    optimizer.step()  # Update the gradient\n",
    "\n",
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9ff34-4b5b-43d1-a1a2-4c101ff4e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Fare'] = test_data['Fare'].map(lambda x: np.log(x) if x > 0 else 0)\n",
    "\n",
    "new_test_data = test_data.drop(2, axis = 1)\n",
    "new_test_data = new_test_data.drop(3, axis = 1)\n",
    "new_test_data = new_test_data.drop(4, axis = 1)\n",
    "new_test_data = new_test_data.drop('SibSp', axis = 1)\n",
    "\n",
    "\n",
    "correct=0\n",
    "wrong = 0\n",
    "df_selected = new_test_data.iloc[:100]\n",
    "X = df_selected.iloc[:, :-1].values\n",
    "y = df_selected.iloc[:, -1].values\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "pred_y = network.forward(X_tensor)\n",
    "pred = (pred_y >= 0.5).float()  # Since the answers are either 0 or 1, we need to set a threshold where >= 0.5 is 1 and < 0.5 is 0\n",
    "for i in range(100):\n",
    "    if pred[i].item() == y_tensor[i].item():\n",
    "        correct += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "\n",
    "print (correct/(correct + wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd520676-64ea-4c7f-a95d-eaa29b8f4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "\n",
    "\n",
    "df_selected = train_data.iloc[:500]\n",
    "X = df_selected.iloc[:, :-1].values\n",
    "y = df_selected.iloc[:, -1].values\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "df_test_selected = test_data.iloc[:100]\n",
    "X_test = df_test_selected.iloc[:, :-1].values\n",
    "y_test = df_test_selected.iloc[:, -1].values\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_tensor, y_tensor)\n",
    "Y_pred = logreg.predict(X_test_tensor)\n",
    "acc_log = round(logreg.score(X_test_tensor, y_test_tensor) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f656e-2c66-4891-97e7-fe1c1ba71f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_tensor, y_tensor)\n",
    "Y_pred = perceptron.predict(X_tensor)\n",
    "acc_perceptron = round(perceptron.score(X_test_tensor, y_test_tensor) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9d815-c87a-4708-a0ec-773cba3f32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_tensor, y_tensor)\n",
    "Y_pred = decision_tree.predict(X_tensor)\n",
    "acc_decision_tree = round(decision_tree.score(X_test_tensor, y_test_tensor) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14381a-fe0b-4250-b787-c6e8a0140527",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "random_forest.fit(X_tensor, y_tensor)\n",
    "# Y_pred = random_forest.predict(X_test)\n",
    "acc_random_forest = round(random_forest.score(X_test_tensor, y_test_tensor) * 100, 2)\n",
    "acc_random_forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
