{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "860a5888-fac4-4367-9491-dce6f61ed262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CocoDetection\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa3a6e2e-22c4-4c11-9c22-4e387256bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhil/Workspace/ML_venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nikhil/Workspace/ML_venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 7, 7])\n",
      "torch.Size([1, 256, 7, 7])\n",
      "Backbone output shape: torch.Size([1, 256, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, name=\"resnet50\", pretrained=False, num_channels=256):\n",
    "        super(Backbone, self).__init__()\n",
    "        # Load a ResNet backbone\n",
    "        resnet = getattr(models, name)(pretrained=pretrained)\n",
    "        # Use layers until layer4 as the backbone\n",
    "        self.body = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4,\n",
    "        )\n",
    "        # Reduce dimensionality from 2048 to the specified number of channels (256)\n",
    "        self.conv = nn.Conv2d(2048, num_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.body(x)  # Extract features from ResNet\n",
    "        print(features.shape)\n",
    "        features = self.conv(features)  # Reduce channels to 256\n",
    "        print(features.shape)\n",
    "        return features\n",
    "\n",
    "\n",
    "# Instantiate the backbone\n",
    "backbone = Backbone()\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Simulated input\n",
    "features = backbone(input_tensor)\n",
    "print(\"Backbone output shape:\", features.shape)  # Expected: (1, 256, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acad288e-3232-42ba-bbb5-9010e8b2dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_channels=256, height=50, width=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(num_channels, height, width)\n",
    "        y_pos = torch.arange(0, height, dtype=torch.float32).unsqueeze(1).repeat(1, width)\n",
    "        x_pos = torch.arange(0, width, dtype=torch.float32).unsqueeze(0).repeat(height, 1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, num_channels, 2).float() * (-math.log(10000.0) / num_channels))\n",
    "        pe[0::2, :, :] = torch.sin(y_pos.unsqueeze(0) * div_term[:, None, None])\n",
    "        pe[1::2, :, :] = torch.cos(x_pos.unsqueeze(0) * div_term[:, None, None])\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input feature map of shape (batch_size, num_channels, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, height, width = x.size()\n",
    "        assert num_channels == self.num_channels, \"Feature map channels must match positional encoding channels.\"\n",
    "        assert height <= self.height and width <= self.width, \"Feature map spatial size exceeds positional encoding.\"\n",
    "\n",
    "        return x + self.pe[:, :height, :width]\n",
    "\n",
    "pos_enc = PositionalEncoding()\n",
    "\n",
    "# Test the Transformer\n",
    "features = torch.randn(1, 256, 7, 7)  # Backbone feature map\n",
    "pos_encoded_features = pos_enc(features)  # Add positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05da24ec-e5c1-4c0e-a5fe-42a3372cc4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2500, 1, 256])\n",
      "Transformer output shape: torch.Size([100, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model=256, num_queries=100, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead), num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead), num_layers=num_decoder_layers\n",
    "        )\n",
    "        self.query_embed = nn.Embedding(num_queries, d_model)  # Learnable queries (100 queries for 100 objects)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        \"\"\"\n",
    "        src: Feature map from the backbone (flattened)\n",
    "        mask: Optional mask for padding\n",
    "        \"\"\"\n",
    "        feature_map = self.encoder(src, src_key_padding_mask=mask)  # Transformer encoder output. Feature map. \n",
    "        print(feature_map.shape)\n",
    "        queries = self.query_embed.weight.unsqueeze(1).repeat(1, src.size(1), 1)  # Shape: (num_queries, batch, d_model)\n",
    "        output = self.decoder(queries, feature_map)  # Decoder maps queries to final outputs\n",
    "        return output\n",
    "\n",
    "transformer = Transformer()\n",
    "pos_encoded_features = torch.randn(1, 256, 50, 50)\n",
    "flattened_features = pos_encoded_features.flatten(2).permute(2, 0, 1)  # (H*W, Batch, Channels)\n",
    "\n",
    "output = transformer(flattened_features)  # Transformer output\n",
    "print(\"Transformer output shape:\", output.shape)  # (num_queries, batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51aa66bb-4036-4e5a-bd9f-a3eb00a1a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, d_model=256, num_classes=91):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        \n",
    "        # Classification head: Predicts class probabilities for each query\n",
    "        self.class_head = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        # Bounding box head: Predicts [cx, cy, w, h] for each query\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 4)  # Final output: 4 bounding box coordinates\n",
    "        )\n",
    "    \n",
    "    def forward(self, decoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_outputs: Tensor of shape (num_queries, batch_size, d_model)\n",
    "        \"\"\"\n",
    "        # Classification predictions\n",
    "        class_logits = self.class_head(decoder_outputs)  # (num_queries, batch_size, num_classes)\n",
    "        \n",
    "        # Bounding box predictions\n",
    "        bbox_coords = self.bbox_head(decoder_outputs)   # (num_queries, batch_size, 4)\n",
    "        bbox_coords = bbox_coords.sigmoid()  # Normalize box coordinates to [0, 1]\n",
    "        \n",
    "        return class_logits, bbox_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a62e4241-8bcf-496d-81fb-e683b167dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DETR(nn.Module):\n",
    "    def __init__(self, num_classes=91, num_queries=100, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        DETR Model integrating:\n",
    "        - Backbone for feature extraction.\n",
    "        - Positional Encoding.\n",
    "        - Transformer Encoder-Decoder.\n",
    "        - Detection Head for bounding box and class prediction.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of object classes.\n",
    "            num_queries (int): Number of object queries.\n",
    "            hidden_dim (int): Transformer embedding dimension.\n",
    "        \"\"\"\n",
    "        super(DETR, self).__init__()\n",
    "\n",
    "        # Use the previously defined Backbone class\n",
    "        self.backbone = Backbone(num_channels=hidden_dim)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
    "\n",
    "        # Transformer Encoder-Decoder\n",
    "        self.transformer = Transformer(d_model=hidden_dim, num_queries=num_queries)\n",
    "\n",
    "        # Detection Head (Classification + Bounding Box Regression)\n",
    "        self.detection_head = DetectionHead(hidden_dim, num_classes)\n",
    "\n",
    "        # Object Queries: Learnable embeddings\n",
    "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward pass through DETR:\n",
    "        1. Extract features using CNN backbone.\n",
    "        2. Add positional encoding.\n",
    "        3. Process features through Transformer encoder-decoder.\n",
    "        4. Decode predictions into class probabilities and bounding boxes.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): Batch of images (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            dict: {'pred_logits': class probabilities, 'pred_boxes': bounding boxes}\n",
    "        \"\"\"\n",
    "        # Extract features using Backbone\n",
    "        features = self.backbone(images)  # Shape: (B, 256, H', W')\n",
    "\n",
    "        # Add positional encoding\n",
    "        pos_encoded_features = self.positional_encoding(features)\n",
    "\n",
    "        # Flatten feature map for Transformer\n",
    "        batch_size, _, height, width = features.shape\n",
    "        src = pos_encoded_features.flatten(2).permute(2, 0, 1)  # Shape: (H'*W', B, hidden_dim)\n",
    "\n",
    "        # Create object queries\n",
    "        queries = self.query_embed.weight.unsqueeze(1).repeat(1, batch_size, 1)  # Shape: (num_queries, B, hidden_dim)\n",
    "\n",
    "        # Pass through Transformer\n",
    "        transformer_output = self.transformer(src, queries)  # Shape: (num_queries, B, hidden_dim)\n",
    "\n",
    "        # Convert Transformer output into predictions\n",
    "        class_logits, bbox_coords = self.detection_head(transformer_output)  # (num_queries, B, num_classes), (num_queries, B, 4)\n",
    "\n",
    "        return {\"pred_logits\": class_logits.permute(1, 0, 2), \"pred_boxes\": bbox_coords.permute(1, 0, 2)}\n",
    "\n",
    "# Instantiate the model\n",
    "model = DETR(num_classes=91, num_queries=100)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1faa3ba9-1d6e-475a-8264-a7b5e31f189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class logits shape: torch.Size([100, 1, 91])\n",
      "Bounding box coordinates shape: torch.Size([100, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "d_model = 256\n",
    "num_classes = 91\n",
    "num_queries = 100\n",
    "\n",
    "# Instantiate the Transformer and Detection Heads\n",
    "transformer = Transformer(d_model=d_model)\n",
    "detection_head = DetectionHead(d_model=d_model, num_classes=num_classes)\n",
    "\n",
    "# Simulated input\n",
    "features = torch.randn(1, 256, 50, 50)  # Backbone feature map\n",
    "pos_encoded_features = pos_enc(features)\n",
    "flattened_features = pos_encoded_features.flatten(2).permute(2, 0, 1)  # (H*W, batch, d_model)\n",
    "\n",
    "# Pass through Transformer\n",
    "decoder_output = transformer(flattened_features)  # (num_queries, batch, d_model)\n",
    "\n",
    "# Pass through Detection Head\n",
    "class_logits, bbox_coords = detection_head(decoder_output)\n",
    "\n",
    "print(\"Class logits shape:\", class_logits.shape)  # (num_queries, batch, num_classes)\n",
    "print(\"Bounding box coordinates shape:\", bbox_coords.shape)  # (num_queries, batch, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2aac46-6e13-4b78-95a0-75359d76e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generalized IoU matrix between predictions and ground truth:\n",
      "tensor([[-0.5531, -0.0137, -0.5323, -0.3554, -0.0014],\n",
      "        [ 0.2663, -0.2661, -0.1653, -0.5216, -0.4692],\n",
      "        [-0.1817,  0.0229, -0.1699, -0.4190, -0.2534],\n",
      "        [ 0.0293, -0.0562, -0.2851, -0.5267, -0.1449],\n",
      "        [ 0.2996,  0.0889,  0.2839, -0.2208, -0.1199]])\n"
     ]
    }
   ],
   "source": [
    "# Function to convert bounding boxes from [cx, cy, w, h] to [x_min, y_min, x_max, y_max]\n",
    "def box_cxcywh_to_xyxy(box):\n",
    "    x_c, y_c, w, h = box.unbind(-1)\n",
    "    x_min = x_c - 0.5 * w\n",
    "    y_min = y_c - 0.5 * h\n",
    "    x_max = x_c + 0.5 * w\n",
    "    y_max = y_c + 0.5 * h\n",
    "    return torch.stack([x_min, y_min, x_max, y_max], dim=-1)\n",
    "\n",
    "def generalized_iou(pred_boxes, target_boxes):\n",
    "    \"\"\"\n",
    "    Compute Generalized IoU between predicted and target boxes.\n",
    "    \"\"\"\n",
    "    # Convert [cx, cy, w, h] to [x_min, y_min, x_max, y_max]\n",
    "    pred_boxes = box_cxcywh_to_xyxy(pred_boxes)\n",
    "    target_boxes = box_cxcywh_to_xyxy(target_boxes)\n",
    "\n",
    "    # Compute areas\n",
    "    pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "    target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
    "\n",
    "    # Compute intersection\n",
    "    inter_min = torch.max(pred_boxes[:, None, :2], target_boxes[:, :2])  # (num_queries, num_targets, 2)\n",
    "    inter_max = torch.min(pred_boxes[:, None, 2:], target_boxes[:, 2:])  # (num_queries, num_targets, 2)\n",
    "    inter_wh = (inter_max - inter_min).clamp(min=0)  # (num_queries, num_targets, 2)\n",
    "    inter_area = inter_wh[:, :, 0] * inter_wh[:, :, 1]\n",
    "\n",
    "    # Compute union\n",
    "    union_area = pred_area[:, None] + target_area - inter_area\n",
    "\n",
    "    # IoU\n",
    "    iou = inter_area / union_area\n",
    "\n",
    "    # Compute enclosing box\n",
    "    enclose_min = torch.min(pred_boxes[:, None, :2], target_boxes[:, :2])\n",
    "    enclose_max = torch.max(pred_boxes[:, None, 2:], target_boxes[:, 2:])\n",
    "    enclose_wh = (enclose_max - enclose_min).clamp(min=0)\n",
    "    enclose_area = enclose_wh[:, :, 0] * enclose_wh[:, :, 1]\n",
    "\n",
    "    # Generalized IoU\n",
    "    giou = iou - (enclose_area - union_area) / enclose_area\n",
    "    return giou\n",
    "\n",
    "# Generate random predicted and ground truth boxes\n",
    "pred_boxes = torch.rand(5, 4)\n",
    "gt_boxes = torch.rand(5, 4)\n",
    "\n",
    "# Convert to IoU-compatible format\n",
    "pred_boxes_xyxy = box_cxcywh_to_xyxy(pred_boxes)\n",
    "gt_boxes_xyxy = box_cxcywh_to_xyxy(gt_boxes)\n",
    "\n",
    "# Compute Generalized IoU\n",
    "giou = generalized_iou(pred_boxes_xyxy, gt_boxes_xyxy)\n",
    "\n",
    "print(\"\\nGeneralized IoU matrix between predictions and ground truth:\")\n",
    "print(giou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a3a4202-9fd2-4576-8e20-b03df7d68289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    def __init__(self, cost_class=1, cost_bbox=1, cost_giou=1):\n",
    "        super(HungarianMatcher, self).__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Perform Hungarian matching.\n",
    "        outputs: Dictionary with 'pred_logits' and 'pred_boxes' from the detection head.\n",
    "        targets: List of dictionaries with 'labels' and 'boxes' for each image.\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['pred_logits']  # (batch_size, num_queries, num_classes)\n",
    "        pred_boxes = outputs['pred_boxes']    # (batch_size, num_queries, 4)\n",
    "\n",
    "        batch_size = pred_logits.shape[0]\n",
    "        indices = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # Extract predictions and ground truth for this batch\n",
    "            logits = pred_logits[b]  # (num_queries, num_classes)\n",
    "            boxes = pred_boxes[b]    # (num_queries, 4)\n",
    "            tgt_labels = targets[b]['labels']  # (num_objects,)\n",
    "            tgt_boxes = targets[b]['boxes']    # (num_objects, 4)\n",
    "\n",
    "            # Compute classification cost (cross-entropy)\n",
    "            cost_class = -logits[:, tgt_labels].softmax(dim=-1)\n",
    "\n",
    "            # Compute bbox L1 cost\n",
    "            cost_bbox = torch.cdist(boxes, tgt_boxes, p=1)\n",
    "\n",
    "            # Compute Generalized IoU cost\n",
    "            cost_giou = -generalized_iou(boxes, tgt_boxes)\n",
    "\n",
    "            # Combine costs\n",
    "            total_cost = (\n",
    "                self.cost_class * cost_class +\n",
    "                self.cost_bbox * cost_bbox +\n",
    "                self.cost_giou * cost_giou\n",
    "            )\n",
    "\n",
    "            # Solve assignment problem\n",
    "            matched_indices = linear_sum_assignment(total_cost.cpu().numpy())\n",
    "            indices.append((torch.as_tensor(matched_indices[0]), torch.as_tensor(matched_indices[1])))\n",
    "\n",
    "        return indices\n",
    "\n",
    "class DETRLoss(nn.Module):\n",
    "    def __init__(self, matcher, num_classes, weight_dict):\n",
    "        super(DETRLoss, self).__init__()\n",
    "        self.matcher = matcher\n",
    "        self.num_classes = num_classes\n",
    "        self.weight_dict = weight_dict\n",
    "        self.bce = nn.CrossEntropyLoss()\n",
    "        self.l1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        indices = self.matcher(outputs, targets)\n",
    "\n",
    "        # Extract matched predictions and targets\n",
    "        pred_logits = outputs['pred_logits']\n",
    "        pred_boxes = outputs['pred_boxes']\n",
    "\n",
    "        # Initialize losses\n",
    "        loss_class = 0\n",
    "        loss_bbox = 0\n",
    "        loss_giou = 0\n",
    "\n",
    "        for batch_idx, (pred_idx, target_idx) in enumerate(indices):\n",
    "            # Classification loss\n",
    "            target_classes = targets[batch_idx]['labels'][target_idx]\n",
    "            loss_class += self.bce(pred_logits[batch_idx, pred_idx], target_classes)\n",
    "\n",
    "            # Bounding box loss\n",
    "            target_boxes = targets[batch_idx]['boxes'][target_idx]\n",
    "            loss_bbox += self.l1(pred_boxes[batch_idx, pred_idx], target_boxes)\n",
    "            loss_giou += 1 - generalized_iou(pred_boxes[batch_idx, pred_idx], target_boxes).mean()\n",
    "\n",
    "        total_loss = (self.weight_dict['class'] * loss_class +\n",
    "                      self.weight_dict['bbox'] * loss_bbox +\n",
    "                      self.weight_dict['giou'] * loss_giou)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d32b96d2-3ead-4ea3-a9ce-89595351f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to generate random bounding boxes and labels\n",
    "def generate_synthetic_data(batch_size, num_queries, num_objects, num_classes):\n",
    "    \"\"\"\n",
    "    Generates random predictions and ground truth for testing.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of images in the batch.\n",
    "        num_queries: Number of object queries (fixed at 100 in DETR).\n",
    "        num_objects: Number of actual objects in the image.\n",
    "        num_classes: Number of object classes.\n",
    "    \n",
    "    Returns:\n",
    "        predictions: Dict containing 'pred_logits' and 'pred_boxes'.\n",
    "        targets: List of dictionaries containing 'labels' and 'boxes' for each image.\n",
    "    \"\"\"\n",
    "    predictions = {\n",
    "        'pred_logits': torch.randn(batch_size, num_queries, num_classes),  # Random class scores\n",
    "        'pred_boxes': torch.rand(batch_size, num_queries, 4)  # Random bounding boxes in [0,1]\n",
    "    }\n",
    "    \n",
    "    targets = []\n",
    "    for _ in range(batch_size):\n",
    "        num_gt = np.random.randint(1, num_objects + 1)  # Random number of ground truth objects\n",
    "        \n",
    "        labels = torch.randint(1, num_classes, (num_gt,))  # Random class labels (excluding \"no object\" class)\n",
    "        boxes = torch.rand(num_gt, 4)  # Random bounding boxes in [0,1]\n",
    "\n",
    "        targets.append({'labels': labels, 'boxes': boxes})\n",
    "    \n",
    "    return predictions, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7897db7b-a1f8-4a33-bc3d-28747d40d143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 synthetic samples with max 5 objects per image.\n",
      "\n",
      "Image 0:\n",
      "  Matched predictions (query indices): [3]\n",
      "  Matched ground truth objects (target indices): [0]\n",
      "\n",
      "Image 1:\n",
      "  Matched predictions (query indices): [4, 19, 44]\n",
      "  Matched ground truth objects (target indices): [0, 1, 2]\n",
      "\n",
      "Computed DETR loss: 9.935282707214355\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "batch_size = 2\n",
    "num_queries = 100\n",
    "num_objects = 5\n",
    "num_classes = 10\n",
    "\n",
    "predictions, targets = generate_synthetic_data(batch_size, num_queries, num_objects, num_classes)\n",
    "\n",
    "print(f\"Generated {batch_size} synthetic samples with max {num_objects} objects per image.\")\n",
    "\n",
    "# Instantiate the Hungarian matcher\n",
    "matcher = HungarianMatcher(cost_class=1, cost_bbox=1, cost_giou=1)\n",
    "\n",
    "# Perform matching\n",
    "matched_indices = matcher(predictions, targets)\n",
    "\n",
    "# Print results\n",
    "for batch_idx, (pred_idx, target_idx) in enumerate(matched_indices):\n",
    "    print(f\"\\nImage {batch_idx}:\")\n",
    "    print(f\"  Matched predictions (query indices): {pred_idx.tolist()}\")\n",
    "    print(f\"  Matched ground truth objects (target indices): {target_idx.tolist()}\")\n",
    "\n",
    "# Define loss weightings\n",
    "weight_dict = {'class': 1, 'bbox': 5, 'giou': 2}\n",
    "\n",
    "# Instantiate loss function\n",
    "detr_loss = DETRLoss(matcher, num_classes, weight_dict)\n",
    "\n",
    "# Compute loss\n",
    "loss = detr_loss(predictions, targets)\n",
    "\n",
    "print(\"\\nComputed DETR loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51529a48-f582-4602-8d22-9fee54a9382f",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232bfb9-1c7d-4dfb-9942-523ef805a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Download and load VOC dataset (trainval split)\n",
    "train_dataset = datasets.VOCDetection(\n",
    "    root='./data',\n",
    "    year='2012',\n",
    "    image_set='val',  # You can also use 'train' or 'val'\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Example for running inference and visualizing\n",
    "image, target = train_dataset[0]\n",
    "print(image.shape)  # Example image shape (3, 800, 800)\n",
    "print(target)  # Annotations (bounding boxes, labels, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57af56-ae69-4edf-b205-dd24683f499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_voc_sample(dataset, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualizes a few images from the VOC dataset with their bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The VOC dataset.\n",
    "        num_samples: Number of images to visualize.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        img, target = dataset[i]\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert from Tensor to NumPy format\n",
    "\n",
    "        # Get bounding boxes and labels\n",
    "        boxes = target[\"annotation\"][\"object\"]\n",
    "        if isinstance(boxes, dict):  # If only one object, wrap it in a list\n",
    "            boxes = [boxes]\n",
    "\n",
    "        fig, ax = plt.subplots(1, figsize=(6, 6))\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        for obj in boxes:\n",
    "            bbox = obj[\"bndbox\"]\n",
    "            x_min, y_min, x_max, y_max = map(int, [bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]])\n",
    "            label = obj[\"name\"]\n",
    "\n",
    "            # Draw bounding box\n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                linewidth=2, edgecolor=\"r\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x_min, y_min - 5, label, color=\"red\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "visualize_voc_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e834c9-6462-43f0-9948-03e2c6f77de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Assume `DETRModel` is your DETR implementation (replace with your model)\n",
    "model = DETR()  # Replace with your DETR model initialization\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "\n",
    "# Loss function (assuming it's implemented)\n",
    "detr_loss = YourDETRLossFunction()  # Replace with actual DETR loss function\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Convert VOC targets to DETR format (bounding boxes and labels)\n",
    "            boxes, labels = [], []\n",
    "            for target in targets:\n",
    "                objects = target[\"annotation\"][\"object\"]\n",
    "                if isinstance(objects, dict):  # If only one object, wrap it in a list\n",
    "                    objects = [objects]\n",
    "                \n",
    "                img_boxes = []\n",
    "                img_labels = []\n",
    "                for obj in objects:\n",
    "                    bbox = obj[\"bndbox\"]\n",
    "                    x_min, y_min, x_max, y_max = map(int, [bbox[\"xmin\"], bbox[\"ymin\"], bbox[\"xmax\"], bbox[\"ymax\"]])\n",
    "                    img_boxes.append([x_min, y_min, x_max - x_min, y_max - y_min])  # Convert to (x, y, w, h)\n",
    "                    img_labels.append(int(obj[\"name\"]))\n",
    "\n",
    "                boxes.append(torch.tensor(img_boxes, dtype=torch.float32).to(device))\n",
    "                labels.append(torch.tensor(img_labels, dtype=torch.int64).to(device))\n",
    "\n",
    "            # Create DETR-style targets\n",
    "            targets = [{\"boxes\": b, \"labels\": l} for b, l in zip(boxes, labels)]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = detr_loss(outputs, targets)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc401e2-d90d-47e4-8a65-7c4d973212aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def visualize_attention(model, image, feature_map, decoder_attention_weights, query_idx):\n",
    "    \"\"\"\n",
    "    Visualizes the attention map for a specific query on an input image.\n",
    "    \n",
    "    Args:\n",
    "        model: The DETR model.\n",
    "        image: The input image (C, H, W).\n",
    "        feature_map: The encoded feature map from the backbone.\n",
    "        decoder_attention_weights: The cross-attention weights from the decoder.\n",
    "        query_idx: Index of the query to visualize.\n",
    "    \"\"\"\n",
    "    # Extract attention map for the given query\n",
    "    # Attention weights: (num_queries, num_heads, H * W)\n",
    "    query_attention = decoder_attention_weights[0, query_idx].mean(dim=0)  # Average across heads\n",
    "    query_attention = query_attention.view(feature_map.shape[2], feature_map.shape[3])  # Reshape to (H, W)\n",
    "    \n",
    "    # Normalize attention for visualization\n",
    "    query_attention = query_attention.detach().cpu().numpy()\n",
    "    query_attention = (query_attention - query_attention.min()) / (query_attention.max() - query_attention.min())\n",
    "\n",
    "    # Upsample attention to input image size\n",
    "    attention_resized = F.resize(torch.tensor(query_attention), size=(image.shape[1], image.shape[2]), interpolation=F.InterpolationMode.BILINEAR)\n",
    "\n",
    "    # Convert image to NumPy for visualization\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())  # Normalize to [0, 1]\n",
    "\n",
    "    # Overlay attention map on the image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_np)\n",
    "    plt.imshow(attention_resized.numpy(), alpha=0.5, cmap='jet')  # Overlay with transparency\n",
    "    plt.title(f\"Attention Map for Query {query_idx}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9f038-07cb-4234-a92c-62bd6745ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "def visualize_self_attention(feature_map, attention_weights, spatial_idx, input_image):\n",
    "    \"\"\"\n",
    "    Visualizes self-attention for a specific spatial position in the feature map.\n",
    "    \n",
    "    Args:\n",
    "        feature_map: The feature map (batch_size, C, H, W).\n",
    "        attention_weights: Self-attention weights (H * W, H * W).\n",
    "        spatial_idx: The index of the spatial position to visualize (row * W + col).\n",
    "        input_image: The input image (C, H, W).\n",
    "    \"\"\"\n",
    "    # Feature map spatial dimensions\n",
    "    H, W = feature_map.shape[2], feature_map.shape[3]\n",
    "    \n",
    "    # Extract attention for the chosen spatial position\n",
    "    attention = attention_weights[spatial_idx]  # (H * W,)\n",
    "    attention = attention.view(H, W)  # Reshape to feature map shape\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    attention = attention.detach().cpu().numpy()\n",
    "    attention = (attention - attention.min()) / (attention.max() - attention.min())\n",
    "    \n",
    "    # Upsample attention to input image size\n",
    "    attention_resized = F.resize(torch.tensor(attention), size=(input_image.shape[1], input_image.shape[2]), interpolation=F.InterpolationMode.BILINEAR)\n",
    "    \n",
    "    # Convert input image to NumPy\n",
    "    input_image_np = input_image.permute(1, 2, 0).cpu().numpy()\n",
    "    input_image_np = (input_image_np - input_image_np.min()) / (input_image_np.max() - input_image_np.min())\n",
    "    \n",
    "    # Plot the attention map\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(input_image_np)\n",
    "    plt.imshow(attention_resized.numpy(), alpha=0.5, cmap='jet')  # Overlay attention map\n",
    "    plt.title(f\"Self-Attention for Position {spatial_idx} (Feature Map)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Suppose `feature_map` is (1, C, H, W), `attention_weights` is (H*W, H*W), and `input_image` is (C, H_img, W_img)\n",
    "# spatial_idx = 50  # Choose a spatial index\n",
    "# visualize_self_attention(feature_map, attention_weights, spatial_idx, input_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
